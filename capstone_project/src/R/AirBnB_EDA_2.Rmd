---
title: "AirBnB Exploratory Data Analysis, part 2"
output: github_document
---


```{r}

library(tidyverse)
library(gridExtra)
library(broom)
library(GGally)
library(ggcorrplot)
library(reshape2)
library(xgboost)
library(caret)

library(geosphere)

airBColor<- '#ff5a5f'
airBColor2<- '#008489'
```

```{r}
df<-read.csv('data/listing_cleansed.csv',stringsAsFactors = FALSE)

print (table(sapply(df, class)))

df <-df %>% select(-which(sapply(.,is.character))) 
df <- df %>% filter(availability_90>0)

df$brandenburg_tor_lat <-52.516849
df$brandenburg_tor_lon <-13.377661
df$distance <- distVincentyEllipsoid(df[,c('longitude','latitude')], df[,c('brandenburg_tor_lon','brandenburg_tor_lat')])

df<- df %>% mutate(target = ifelse(availability_90<30,1,0))

print (table(sapply(df, class)))
```


```{r}
ggplot(df, aes(x=target))+geom_histogram()

ggplot(df , aes(y=distance,x=review_scores_rating))+geom_point(alpha=.5,aes(colour = target)) +scale_colour_gradient(low =airBColor, high =airBColor2)+theme_minimal()+theme_set(theme_get() + theme(text = element_text(family = 'CMU Serif')))+ ggtitle("This is a non-default font")


```


```{r}

lm<-lm(availability_90~., df  %>% select(-one_of(c('availability_30','availability_60','availability_365','target'))))
summary(lm)$adj.r.squared
top_corr<- tidy(lm) %>% filter(p.value<.05)  %>% top_n(n = -200,wt = p.value) %>% arrange(p.value) %>% pull(term)
length(top_corr)
```

```{r}
df_listing_top_features <- df %>% select(one_of(c(top_corr,'availability_90')))
ggplot(data = melt(df_listing_top_features), mapping = aes(x = value)) + 
    geom_histogram(bins = 60, fill=airBColor) + facet_wrap(~variable, scales = 'free')
```



```{r}
corr <- round(cor(df %>% select(one_of(c(top_corr,'availability_90') %>% na.omit() ))),3)


ggcorrplot(corr, hc.order = FALSE, 
           type = "lower", 
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("red", "white", "blue"), 
           title="Correlogram of Selected variables", 
           ggtheme=theme_bw,
           tl.cex = 6)
```
```{r}
ggplot(df %>% filter(distance<10000),aes(availability_90,distance))+ geom_point(alpha=.2,color=airBColor2)+geom_smooth()
cor(na.omit(df)$distance,na.omit(df)$price)
```
# see https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees
```{r}

#set.seed(124)

df_no_text<- df %>% na.omit()



X<- data.matrix(df_no_text %>% select(-one_of(c('id','availability_30','availability_90','availability_365','availability_60','availability_90','target'))))

print (dim(X))

y<- data.matrix(df_no_text$target)

a <- createDataPartition(iris$Species, p = 0.8, list=FALSE)
X_train <- X[a,]
X_test <- X[-a,]

y_train = y[a,]
y_test = y[-a,]


bst <- xgboost(data =  X_train, label = y_train, max_depth = 12, 
               eta = 0.1, nthread = 2, nrounds = 5,objective = "binary:logistic")

df.importances<-xgb.importance(colnames(X_train), model = bst)

prediction <- predict(bst,X_test)

CM<- table(y_test,prediction>0.5)
CM

sum(diag(CM))/sum(CM)


#postResample(pred = prediction, obs = y_test)


ggplot(df.importances ,aes(reorder(Feature, -Gain),Gain))+geom_bar(stat = 'identity',fill=airBColor2)+theme(axis.text.x=element_text(angle=90,hjust=1)) 
```
```{r}


# xgboost fitting with arbitrary parameters
xgb_params_1 = list(
  objective = "binary:logistic",                                               # binary classification
  eta = 0.01,                                                                  # learning rate
  max.depth = 6,                                                               # max tree depth
  eval_metric = "auc"                                                          # evaluation/loss metric
)



```

```{r}
# cross-validate xgboost to get the accurate measure of error
xgb_cv_1 <- xgb.cv(data = X_train,
                label = y_train,
                params = xgb_params_1,
                  nrounds = 100, 
                  nfold = 5,                                                   # number of folds in K-fold
                  prediction = TRUE,                                           # return the prediction using the final model 
                  showsd = TRUE,                                               # standard deviation of loss across folds
                  stratified = TRUE,                                           # sample is unbalanced; use stratified sampling
                  verbose = TRUE,
                  print_every_n = 1, 
                  early_stopping_rounds = 10)


df.importances<-xgb.importance(colnames(X_train), model = xgb_cv_1)
# plot the AUC for the training and testing samples
ggplot(df.importances ,aes(reorder(Feature, -Gain),Gain))+geom_bar(stat = 'identity',fill=airBColor2)+theme(axis.text.x=element_text(angle=90,hjust=1)) 
```


